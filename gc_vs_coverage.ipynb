{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b318748",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "raw",
   "id": "449dfc15",
   "metadata": {},
   "source": [
    "1. Get GC content from contigs and in chunk sizes, then visualize both\n",
    "2. Get coverage for contigs and in chunk sizes, then visualize both \n",
    "3. Histogram of contig lengths\n",
    "4. Blob plot (gc vs average coverage) for both\n",
    "5. Kmeans clustering\n",
    "6. Take cluster from kmeans as input to Blast_script\n",
    "7. get concencus classification for each contig\n",
    "8. Blob plot again and mark points that had accurate blast hits\n",
    "9. Histogram for length distribution\n",
    "10. Take contigs starting with largest up til genome size\n",
    "11. Add contig ids to fasta file for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7684a0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'multiprocess'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiprocess\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'multiprocess'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import multiprocess\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2893ee",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0ec1c42",
   "metadata": {},
   "source": [
    "#PARAMETERS\n",
    "# assembly : The fasta input assembly file\n",
    "# coverage_per_contig : A tsv file with information for each contig\n",
    "# coverage_per_base:  A tsv file with coverage information for each base pair position\n",
    "# n_cores: number of cpu cores to multiprocess with\n",
    "# out_dir : output directory\n",
    "# min_contig_size : The minimum size of a contig, otherwise its ignored\n",
    "# chunk_size : The chunk size for dividing up longer contigs (default 10000)\n",
    "# visualize_gc : An option for creating plots showing the distribution of GC content accross a contig in steps of a given chunk size (default = True)\n",
    "# visualize_coverage : An option to create plots showing coverage distribution per contig\n",
    "# visualize_histogram : Option to cisualize histogram of conitg lengths\n",
    "# threshold_pct : The percentage away from the mean that something needs to be in order to be highlighted\n",
    "# visualize_blob : Option to visualize blob plot\n",
    "# plot_min_length : The minimum contig length to be plotted in the Blob plot\n",
    "# n_clusters : Number of clusters for kmeans\n",
    "# len_to_blast : Length of the to be blasted subsections\n",
    "# space_to_blast : Distance between subsections to be blasted\n",
    "# threshold for what % of classification is enough to call that contig\n",
    "# genome_size : Used to take longest contigs up to genome size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1180c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = \"./data/cladorhizid_v0.6_hapA/clado_v0.6_hapA.fasta\"\n",
    "coverage_per_contig = \"./data/cladorhizid_hifi_assembly.coverage\"\n",
    "coverage_per_base = \"./data/cladorhizid_hifi_assembly.depth\"\n",
    "n_cores = 2\n",
    "out_dir = \"out_dir/\"\n",
    "min_contig_size = 100000\n",
    "chunk_size = 10000\n",
    "visualize_gc = False\n",
    "visualize_coverage = False\n",
    "visualize_histogram = True\n",
    "threshold_pct = 50\n",
    "visualize_blob = True\n",
    "plot_min_length = 100000\n",
    "n_clusters = 2\n",
    "len_to_blast = 1000\n",
    "space_to_blast = 10000\n",
    "threshold = 0.8\n",
    "genome_size = 430000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2a78d",
   "metadata": {},
   "source": [
    "# Create outdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a593f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = out_dir + \"cluster_info/\"\n",
    "coverage_output = out_dir + \"Coverage_output/\"\n",
    "GC_visual_output = out_dir + \"GC_visual_output/\"\n",
    "if not os.path.exists(cluster_info):\n",
    "            os.makedirs(cluster_info)\n",
    "if not os.path.exists(coverage_output):\n",
    "            os.makedirs(coverage_output)\n",
    "if not os.path.exists(GC_visual_output):\n",
    "            os.makedirs(GC_visual_output)\n",
    "cluster_info = ''\n",
    "coverage_output = ''\n",
    "GC_visual_output = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b434043",
   "metadata": {},
   "source": [
    "# Naive GC content for each content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff775cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content(fasta_file):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to their GC content\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq y min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:\n",
    "                        gc_content[current_id] = (current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq)\n",
    "                        \n",
    "                # Start the new contig\n",
    "                #current_id = line.strip()[1:].split(\"/\")[3]\n",
    "                current_id = line.strip().strip('>')\n",
    "                current_seq = \"\"\n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "    \n",
    "    # Calculate GC content for the final contig\n",
    "    gc_content[current_id] = (current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq)\n",
    "    \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c675f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_gc = get_gc_content(assembly)\n",
    "#print(naive_gc)\n",
    "print(len(naive_gc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad90d78",
   "metadata": {},
   "source": [
    "# GC content for each contig by averaging in given chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad03fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content_chunks(fasta_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to a list of gc content per chunk size\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq < min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:  \n",
    "                        i = 0\n",
    "                        while i < len(current_seq):\n",
    "                            # check for last chunk\n",
    "                            if (i + chunk_size) > len(current_seq):\n",
    "                                end = len(current_seq)\n",
    "                            else:\n",
    "                                end = i + chunk_size\n",
    "                            sub = current_seq[i:end]\n",
    "                            gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)),2)\n",
    "                            chunks.append(gc)\n",
    "                            i = end\n",
    "                        # add to dictionary in form {current_id : [array of GC content of given chunk size]}\n",
    "                        gc_content[current_id] = [chunks, len(current_seq)]\n",
    "\n",
    "                # Start the new contig\n",
    "                #current_id = line.strip()[1:].split(\"/\")[3]\n",
    "                current_id = line.strip()\n",
    "                current_seq = \"\"\n",
    "                chunks = []\n",
    "                \n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "                \n",
    "        # calculate GC content for the last contig\n",
    "        if current_id is not None:\n",
    "            i = 0\n",
    "            while i < len(current_seq):\n",
    "                # check for last chunk\n",
    "                if (i + chunk_size) > len(current_seq):\n",
    "                    end = len(current_seq)\n",
    "                else:\n",
    "                    end = i + chunk_size\n",
    "                sub = current_seq[i:end]\n",
    "                gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)), 2)\n",
    "                chunks.append(gc)\n",
    "                i = end\n",
    "            # add to dictionary in form {current_id : [array of GC content of given chunk size, length]}\n",
    "            gc_content[current_id] = [chunks, len(current_seq)]\n",
    "        \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d18eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = get_gc_content_chunks(assembly, chunk_size)\n",
    "gc_trimmed_mean = {}\n",
    "#Get the trimmed mean from chunks\n",
    "for entry in chunks:\n",
    "    length = chunks.get(entry)[1]\n",
    "    avg_cov = chunks.get(entry)[0]\n",
    "    gc_trimmed_mean[entry] = [stats.trim_mean(avg_cov, 0.05), length]\n",
    "print('Length check: ', len(gc_trimmed_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e2afd",
   "metadata": {},
   "source": [
    "# Visualize GC content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b706d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_gc:\n",
    "\n",
    "    save_dir = out_dir + \"/GC_visual_output\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    def process_gc_plot(chunk):\n",
    "        id = chunk\n",
    "        \n",
    "        # added to resume if crashes for whatever reason\n",
    "        save_path = os.path.join(save_dir, \"GC_content_\" + str(id) + \".png\")\n",
    "        \n",
    "        # Skip if the plot already exists\n",
    "        if not os.path.exists(save_path):\n",
    "            gc_content = np.array(chunks.get(chunk)[0])\n",
    "\n",
    "            # create a bar plot\n",
    "            fig, ax = plt.subplots()\n",
    "            mean_gc = np.mean(gc_content)\n",
    "            colors = ['red' if abs(gc - mean_gc) / mean_gc * 100 >= threshold_pct else 'blue' for gc in gc_content]\n",
    "            plt.bar(range(len(gc_content)), gc_content, color=colors)\n",
    "            ax.axhline(y=gc_trimmed_mean[id][0], color='black')\n",
    "            plt.title(\"Contig: \" + str(id))\n",
    "            plt.xlabel('index of Chunk of size: ' + str(chunk_size))\n",
    "            plt.ylabel('GC content')\n",
    "            #save the plot\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f'Skipping {id} (already exists)')\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        with multiprocess.Pool(processes=n_cores) as pool:\n",
    "            print('Starting Visualize GC content')\n",
    "            pool.map(process_gc_plot, chunks.keys())\n",
    "            print('Finished Visualize GC content')\n",
    "\n",
    "else:\n",
    "    print(\"GC visualization is turned off.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a8705",
   "metadata": {},
   "source": [
    "# Naive coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a58bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig(coverage_file):\n",
    "    naive_coverage = {}\n",
    "\n",
    "    with open(coverage_file, \"r\") as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            # Split line into columns\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "            #print(columns)\n",
    "\n",
    "            #only add if length > min_contig_size\n",
    "            if (int(columns[2]) - int(columns[1])) > min_contig_size:\n",
    "                # Use the first column as key and the sixth column as value\n",
    "                key = columns[0]\n",
    "                value = columns[6]\n",
    "                length = int(columns[2]) - int(columns[1])\n",
    "\n",
    "                # Add key-value pair to dictionary\n",
    "                naive_coverage[key] = [float(value), length]\n",
    "            \n",
    "    f.close()\n",
    "    return naive_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_coverage = get_coverage_per_contig(coverage_per_contig)\n",
    "print(len(naive_coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb8a1c",
   "metadata": {},
   "source": [
    "# Chunk averaged Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a896c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig_by_chunks(depth_file, chunk_size):\n",
    "    save_dir = out_dir + \"Coverage_output/\"\n",
    "    \n",
    "    # Define a dictionary to store the coverage data for each ID\n",
    "    id_data = {}\n",
    "\n",
    "    # Read in the TSV file\n",
    "    with open(depth_file, 'r') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "\n",
    "        # Iterate over each row in the file\n",
    "        for i, row in enumerate(reader):\n",
    "            if i % 100000000 == 0:\n",
    "                print(f'reading in line {i}')\n",
    "            # Extract the ID, index, and coverage from the row\n",
    "            id = row[0]\n",
    "            index = int(row[1])\n",
    "            coverage = float(row[2])\n",
    "\n",
    "            # Check if this is the first row for this ID\n",
    "            if id not in id_data:\n",
    "                id_data[id] = []\n",
    "\n",
    "            # Find the window index for this row\n",
    "            window_index = index // chunk_size\n",
    "\n",
    "            # Check if there is already coverage data for this window\n",
    "            if len(id_data[id]) <= window_index:\n",
    "                # Add a new entry for this window\n",
    "                id_data[id].append({'window_sum': coverage, 'window_count': 1})\n",
    "            else:\n",
    "                # Add the coverage to the existing window data\n",
    "                id_data[id][window_index]['window_sum'] += coverage\n",
    "                id_data[id][window_index]['window_count'] += 1            \n",
    "                \n",
    "    memory_info = psutil.Process().memory_info()\n",
    "    print(f\"Read in the info: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "    \n",
    "    # Write the output to a file\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", 'w') as outfile:\n",
    "        outfile.write(\"id\\tindex\\taverage_coverage\\n\")\n",
    "        for id, windows in id_data.items():\n",
    "            #check total length > min_contig_size\n",
    "            total_len = 0\n",
    "            for i, window in enumerate(windows):\n",
    "                total_len += window['window_count']\n",
    "                if total_len > min_contig_size:\n",
    "                    for i, window in enumerate(windows):\n",
    "                        if window['window_count'] > 0:\n",
    "                            avg_coverage = round((window['window_sum'] / window['window_count']), 2)\n",
    "                            outfile.write(f\"{id}\\t{i}\\t{avg_coverage}\\n\")\n",
    "                        \n",
    "        outfile.close()\n",
    "    \n",
    "    #now get the average coverage for each id\n",
    "    # Open the input file\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", \"r\") as f:\n",
    "        next(f)\n",
    "        # Create a dictionary to store the total coverage and highest index for each id\n",
    "        id_coverage = {}\n",
    "        # Read through each line in the file\n",
    "        for line in f:\n",
    "            # Split the line into three columns\n",
    "            id, index, coverage = line.strip().split(\"\\t\")\n",
    "            # Convert the coverage to float\n",
    "            index = float(index)\n",
    "            coverage = float(coverage)\n",
    "            # If the id is not in the dictionary yet, add it with a coverage of 0 and index of -1\n",
    "            if id not in id_coverage:\n",
    "                id_coverage[id] = {\"coverage\": 0, \"max_index\": -1}\n",
    "            # Add the coverage to the total for this id\n",
    "            id_coverage[id][\"coverage\"] += coverage\n",
    "            # Update the max index for this id if the current index is higher\n",
    "            if index > id_coverage[id][\"max_index\"]:\n",
    "                id_coverage[id][\"max_index\"] = index\n",
    "    f.close()\n",
    "    \n",
    "    #make list of coverages, then take trimmed mean of that list\n",
    "    coverage_trimmed_mean = {}\n",
    "    for entry in id_data:\n",
    "        coverages = []\n",
    "        for chunk in id_data.get(entry):\n",
    "            window_sum = float(chunk.get('window_sum'))\n",
    "            window_count = float(chunk.get('window_count'))\n",
    "            coverage = round((window_sum / window_count), 2)\n",
    "            coverages.append(coverage)\n",
    "        \n",
    "        coverage_trimmed_mean[entry] = round(stats.trim_mean(coverages, 0.05), 2)\n",
    "        \n",
    "    # Open the output file\n",
    "    with open(save_dir + \"chunked_coverage.tsv\", \"w\") as f:\n",
    "        # Write the header row\n",
    "        f.write(\"id\\taverage_coverage\\n\")\n",
    "        # Loop through the ids in the dictionary\n",
    "        for id in coverage_trimmed_mean:\n",
    "            average_coverage = coverage_trimmed_mean[id]\n",
    "            f.write(\"{}\\t{}\\n\".format(id, average_coverage))\n",
    "    f.close()\n",
    "\n",
    "    memory_info = psutil.Process().memory_info()\n",
    "    print(f\"Final memory: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "    gc.collect()\n",
    "    memory_info = psutil.Process().memory_info()\n",
    "    print(f\"cleaned up memory: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "    \n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb693cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_coverage = get_coverage_per_contig_by_chunks(coverage_per_base, chunk_size)\n",
    "len(chunks_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc8e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chunks_coverage.get('CLA4_hapA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121ae3a",
   "metadata": {},
   "source": [
    "# Visualize Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b97cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning, for long pacbio reads this uses a LOT of RAM, if your system has less than 16GB beware!\n",
    "\n",
    "# memory_info = psutil.Process().memory_info()\n",
    "# print(f\"Current Memory used: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "if visualize_coverage:\n",
    "    print('Starting Visualize Coverage')\n",
    "    save_dir = out_dir + \"Coverage_output/\"\n",
    "    data = pd.read_csv(save_dir + \"/chunked_windows_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\")\n",
    "    trimmed_mean = pd.read_csv(save_dir + \"/chunked_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\")\n",
    "    trimmed_mean = pd.read_csv(save_dir + \"/chunked_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\")\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert data to pandas dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    mean = pd.DataFrame(trimmed_mean)\n",
    "    # Get a list of unique ids\n",
    "    unique_ids = df['id'].unique()\n",
    "    print('Length of unique ids: ', len(unique_ids))\n",
    "    gc.collect()\n",
    "    \n",
    "    #Plotting\n",
    "    #--------------- \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlabel('Index of chunk size ' + str(chunk_size))\n",
    "    plt.ylabel('Average coverage')\n",
    "\n",
    "    for id in unique_ids:\n",
    "        if id == '000000' or id == '000001':\n",
    "            print('Need more RAM')\n",
    "            continue\n",
    "        print(id)\n",
    "        memory_info = psutil.Process().memory_info()\n",
    "        print(f\"In loop Memory used: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "        \n",
    "        # added to resume if crashes for whatever reason\n",
    "        save_path = os.path.join(save_dir, \"Coverage_\" + str(id) + \".png\")\n",
    "        \n",
    "        # Skip if the plot already exists\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Skipping {id} (already exists)')\n",
    "            continue\n",
    "            \n",
    "        print(f'start {id}')\n",
    "        # Filter the dataframe to get only the rows for this id\n",
    "        search_str = f\"id == '{id}'\"\n",
    "        #print(search_str)\n",
    "        df_id = df.query(search_str)\n",
    "        trimmed_mean_coverage = float(mean.loc[mean['id'] == id, 'average_coverage'])\n",
    "        ax.axhline(y=trimmed_mean_coverage, color='black')\n",
    "        # filtered colors\n",
    "        colors = ['red' if abs(coverage - trimmed_mean_coverage) / trimmed_mean_coverage * 100 >= threshold_pct else 'blue' for coverage in df_id['average_coverage']]\n",
    "        plt.bar(df_id['index'], df_id['average_coverage'], label=f\"id={id}\", color=colors)\n",
    "        plt.savefig(save_path)\n",
    "        plt.clf()\n",
    "        gc.collect()\n",
    "        \n",
    "    print('Finished Visualize Coverage')\n",
    "\n",
    "else :\n",
    "    print(\"Coverage visualization is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d53fe",
   "metadata": {},
   "source": [
    "# Blob Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot gc content (x) vs coverage (y)\n",
    "if visualize_blob:\n",
    "\n",
    "    data = [{'id': k, 'gc_content': v} for k, v in naive_gc.items()]\n",
    "    data2 = [{'id': k, 'average_coverage': v[0], 'length': v[1]} for k, v in naive_coverage.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    df2 = pd.DataFrame(data2)\n",
    "    naive_df = pd.merge(df1, df2, on = 'id')\n",
    "    naive_df = naive_df[naive_df['length'] >= plot_min_length]\n",
    "    #print(naive_df)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([0, 100])\n",
    "    cmap = plt.matplotlib.colormaps.get_cmap('plasma')\n",
    "\n",
    "    sc = plt.scatter(naive_df['average_coverage'], naive_df['gc_content'],  c = naive_df['length'], cmap = cmap)\n",
    "    cbar = plt.colorbar(sc)\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Average Coverage %')\n",
    "    plt.ylabel('GC content %')\n",
    "    plt.title('Blob plot with naive averages')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    ax.legend([sc], ['Length'], loc=\"upper right\")\n",
    "    \n",
    "    # save the plot\n",
    "    save_path = os.path.join(out_dir, \"Blob_naive.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "else :\n",
    "    print(\"Visualize Naive Blob is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42338553",
   "metadata": {},
   "source": [
    "# Visualize trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beac461",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_blob:\n",
    "\n",
    "    data = [{'id': k, 'gc_content': v[0], 'length' : v[1]} for k, v in gc_trimmed_mean.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    trimmed_df = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "    trimmed_df = trimmed_df[trimmed_df['length'] >= plot_min_length]\n",
    "    #print(trimmed_df) \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([0, 100])\n",
    "    cmap = plt.matplotlib.colormaps.get_cmap('plasma')\n",
    "\n",
    "    #labels = list(trimmed_df['id'])\n",
    "    sc1 = plt.scatter(trimmed_df['average_coverage'], trimmed_df['gc_content'], c = trimmed_df['length'], cmap = cmap)\n",
    "    cbar = plt.colorbar(sc1)\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Trimmed Average Coverage %')\n",
    "    plt.ylabel('GC content %')\n",
    "    plt.title('Blob plot with trimmed averages with chunks of ' + str(chunk_size))\n",
    "    ax.legend([sc1], ['Length'], loc=\"upper right\")\n",
    "\n",
    "    # save the plot\n",
    "    save_path = os.path.join(out_dir, \"Blob.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "else :\n",
    "    print(\"Visualize Trimmed Blob is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e549a8",
   "metadata": {},
   "source": [
    "# Histogram of contig lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of all contig lengths\n",
    "\n",
    "if visualize_histogram:\n",
    "    data = trimmed_df[\"length\"]\n",
    "    #print(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    plt.hist(data, bins=50, edgecolor='black')\n",
    "    ax.ticklabel_format(style='plain', axis='x')\n",
    "    \n",
    "    plt.xlabel('Contig Lengths')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of contig lengths')\n",
    "    \n",
    "    # save the plot\n",
    "    save_path = os.path.join(out_dir, \"Histogram_of_lengths.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457296f",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca36761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans to get clusters in blob plots\n",
    "np.random.seed(42)\n",
    "\n",
    "coverage = trimmed_df['average_coverage']\n",
    "gc_content = trimmed_df['gc_content']\n",
    "ids = trimmed_df['id'].values\n",
    "\n",
    "#make a feature matrix\n",
    "X = np.column_stack((coverage, gc_content))\n",
    "\n",
    "#kmeans clusters = n_clusters\n",
    "kmeans = KMeans(n_clusters, random_state = 42, n_init='auto').fit(X)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "#create a dictionary for each cluster\n",
    "cluster_points = {i: [] for i in range(len(centroids))}\n",
    "\n",
    "#add the ids to each cluster in the dictionary\n",
    "for i, label in enumerate(labels):\n",
    "    cluster_points[label].append(ids[i])\n",
    "print('Using this plot you will need to select which cluster is the main cluster for the next step')\n",
    "\n",
    "#look up each point in dataframe to get length\n",
    "for cluster in cluster_points:\n",
    "    total_cluster_length = 0\n",
    "    #for each id for each cluster into a seperate file\n",
    "    with open(out_dir + 'cluster_info/cluster_' + str(cluster) + '.txt', 'w') as out:\n",
    "        for i in cluster_points[cluster]:\n",
    "            #write ids into output\n",
    "            out.write(i + '\\n')\n",
    "            \n",
    "            #get length and total length\n",
    "            length = trimmed_df.loc[trimmed_df['id'] == i, 'length'].iloc[0]\n",
    "            total_cluster_length += length\n",
    "    print('total length of cluster ' + str(cluster) + ' : ' + str(total_cluster_length))\n",
    "\n",
    "    print('Cluster 0: aqua star, Cluster 1: green star')\n",
    "colors = ['aqua', 'lawngreen']\n",
    "#plotting\n",
    "plt.scatter(X[:,0], X[:,1], c = labels)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], marker = '*', s = 100, c = colors)\n",
    "plt.xlabel('Coverage %')\n",
    "plt.ylabel('GC_content %')\n",
    "plt.title('K-means clustering | Coverage vs GC_content')\n",
    "\n",
    "# save the plot\n",
    "save_path = os.path.join(out_dir, \"Kmeans_clustering.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d26540",
   "metadata": {},
   "source": [
    "# Making Fasta with ids in target cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(n_clusters):\n",
    "    #Set target cluster \n",
    "    target_cluster = n\n",
    "    if not os.path.exists(out_dir + 'cluster_info/cluster_' + str(target_cluster)+ '/split_contigs/'):\n",
    "                os.makedirs(out_dir + 'cluster_info/cluster_' + str(target_cluster)+ '/split_contigs/')\n",
    "\n",
    "    #for each id in cluster 0 get the sequence and write into a fasta file\n",
    "        #we already have a fasta file with these ids, we just need to extract only the ones we are interested in\n",
    "    with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '.fasta', 'w') as fasta:\n",
    "        with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '.txt', 'r') as cluster:\n",
    "            for line in cluster:\n",
    "                header = line.strip()\n",
    "                #write header\n",
    "                fasta.write('>' + header + '\\n')\n",
    "                #before next header write line for sequence\n",
    "                #search for the header line in assembly which includes header\n",
    "                with open(assembly, 'r') as big:\n",
    "                    found = False\n",
    "                    for line in big:\n",
    "                        if found:\n",
    "                            sequence = line.strip()\n",
    "                            found = False\n",
    "                            break\n",
    "                        if header in line:\n",
    "                            found = True\n",
    "                big.close()\n",
    "                fasta.write(str(sequence) + '\\n')\n",
    "\n",
    "        cluster.close()\n",
    "    fasta.close()\n",
    "\n",
    "    #Split each entry in fasta file, i.e every 10k take 1k sample\n",
    "    #for each entry in fasta file, if length > 10000 then take small chunk every 10000bp\n",
    "    with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '.fasta', 'r') as fasta:\n",
    "        for line in fasta:\n",
    "            if '>' in line:\n",
    "                #write in split_contigs\n",
    "                #print(line)\n",
    "                line = ''.join(line.split())\n",
    "                contig_name = str(line.replace('>', ''))\n",
    "            else:\n",
    "                with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '/split_contigs/' + contig_name + '.fasta', 'w') as contig_file:\n",
    "                    #split into substrings of len_to_blast every space_to_blast\n",
    "                    length = len(line)\n",
    "                    #print(length)\n",
    "                    if length > (space_to_blast + len_to_blast):\n",
    "                        #loop\n",
    "                        pos = 0\n",
    "                        while pos < length:\n",
    "                            start = pos\n",
    "                            end = pos + len_to_blast\n",
    "                            #header = startpos endpos\n",
    "                            #sequence = sequence\n",
    "                            seq = (line[start:end] + '\\n')\n",
    "                            header = ('>' + str(start) + ' : ' + str(end) + '\\n')\n",
    "                            pos += space_to_blast + len_to_blast\n",
    "                            contig_file.write(header)\n",
    "                            contig_file.write(seq)\n",
    "\n",
    "                contig_file.close()\n",
    "\n",
    "    fasta.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feafc09",
   "metadata": {},
   "source": [
    "# STOP! Run Blast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84feccc",
   "metadata": {},
   "source": [
    "# Adjust Cluster 0 and Cluster 1 to where you saved your Blast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a060f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = './100kb_subset_data/cluster_0/split_contigs/'\n",
    "cluster_1 = './100kb_subset_data/cluster_1/split_contigs/'\n",
    "clusters = [cluster_0, cluster_1]\n",
    "needed = []\n",
    "needed1 = []\n",
    "# designate needed to be what you are looking for\n",
    "\n",
    "#dictionaries\n",
    "octocoral = ['Dendronephthya gigantea', 'Xenia sp. Carnegie-2017']\n",
    "dinoflagellates = ['Breviolum minutum Mf 1.05b.01','Symbiodinium microadriaticum','Symbiodinium sp. clade A Y106',\n",
    "                   'Symbiodinium sp. clade C Y103','Symbiodinium kawagutii','Symbiodinium natans',\n",
    "                   'Symbiodinium sp. CCMP2592','Symbiodinium sp. KB8','Symbiodinium sp. CCMP2456',\n",
    "                   'Symbiodinium pilosum','Symbiodinium necroappetens','Cladocopium goreaui']\n",
    "\n",
    "#for each cluster\n",
    "for cluster in clusters:\n",
    "    all_contigs = {}\n",
    "    concencus_dict = {}\n",
    "    #for file in each cluster\n",
    "    for file in os.listdir(cluster):\n",
    "        filename = file.split('.')[0]\n",
    "        #count_dict has a count of number of each occurence of match\n",
    "        count_dict = {'Breviolum minutum Mf 1.05b.01':0,'Symbiodinium microadriaticum':0,\n",
    "                      'Symbiodinium sp. clade A Y106':0,'Symbiodinium sp. clade C Y103':0,'Symbiodinium kawagutii':0,\n",
    "                      'Symbiodinium natans':0,'Symbiodinium sp. CCMP2592':0,'Symbiodinium sp. KB8':0,\n",
    "                      'Symbiodinium sp. CCMP2456':0,'Symbiodinium pilosum':0,'Symbiodinium necroappetens':0,\n",
    "                      'Cladocopium goreaui':0,'Dendronephthya gigantea':0, 'Xenia sp. Carnegie-2017': 0}\n",
    "        if '.long.tab' in file:\n",
    "            with open(cluster + file, 'r') as f:\n",
    "                #print(filename)\n",
    "                #get header number, get last column\n",
    "                for line in f:\n",
    "                    line = line.split(sep = '\\t')\n",
    "                    #increase count for match\n",
    "                    for entry in count_dict:\n",
    "                        if entry in line[24].strip('\\n'):\n",
    "                            count_dict[entry]+=1\n",
    "            all_contigs[filename] = count_dict\n",
    "            \n",
    "    #get count of octocorral and dinoflagellates\n",
    "    for p in all_contigs:\n",
    "        o = 0\n",
    "        d = 0\n",
    "        for entry in all_contigs[p]:\n",
    "            #print(entry)\n",
    "            if entry in dinoflagellates:\n",
    "                d+=all_contigs[p][entry]\n",
    "            if entry in octocoral:\n",
    "                o+=all_contigs[p][entry]\n",
    "        \n",
    "        #decide (octocoral/dinoflagellates/mixed/none)\n",
    "        decision = 'none'\n",
    "        if d == 0 and o >= 1:\n",
    "            decision = 'octocoral'\n",
    "        if d >= 1 and o ==0 :\n",
    "            decision = 'dinoflagellates'\n",
    "        if d >= 1 and o >= 1:\n",
    "            if (d/o) < threshold:\n",
    "                decision = 'octo'\n",
    "            else:\n",
    "                decision = 'mixed'\n",
    "        #add decision to concensus dict\n",
    "        concencus_dict[p] = decision     \n",
    "    \n",
    "    #print totals of each\n",
    "    octo=dino=mixed=none=0\n",
    "    for key in concencus_dict:\n",
    "        if concencus_dict[key] in 'octocoral':\n",
    "            octo +=1\n",
    "#!!!!!!!!!!!!!!!!Change this to where you want it, if you have other data replace the dicts above too!!!!!!!!!!!!\n",
    "            if n == 0:\n",
    "                needed.append(key)\n",
    "            else:\n",
    "                needed1.append(key)\n",
    "        if concencus_dict[key] in 'dinoflagellates':\n",
    "            dino +=1\n",
    "        if concencus_dict[key] in 'mixed':\n",
    "            mixed +=1\n",
    "        if concencus_dict[key] in 'none':\n",
    "            none +=1\n",
    "#     print('octo: ' + str(octo))\n",
    "#     print('dino: ' + str(dino))\n",
    "#     print('mixed: ' + str(mixed))\n",
    "#     print('none: ' + str(none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to output file\n",
    "with open('./out_dir/cluster_info/needed.txt','w') as out:\n",
    "    for i in needed:\n",
    "        out.write(str(i) + '\\n')\n",
    "out.close()\n",
    "print('length in needed.txt: ', len(needed))\n",
    "with open('./out_dir/cluster_info/needed1.txt','w') as out:\n",
    "    for i in needed1:\n",
    "        out.write(str(i) + '\\n')\n",
    "out.close()\n",
    "print('length in needed.txt: ', len(needed1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295ad53",
   "metadata": {},
   "source": [
    "# Blob Plot with Fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc985a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trimmed_df['id'] = int(trimmed_df['id'])\n",
    "with open('out_dir/cluster_info/needed1.txt','r') as keys:\n",
    "    tot = []\n",
    "    for line in keys:\n",
    "        k = line.strip('\\n')\n",
    "        tot.append(k)\n",
    "print('total length: ', len(tot))\n",
    "     \n",
    "#plot blob again and color contigs with keys as labels\n",
    "\n",
    "data = [{'id': k, 'gc_content': v[0], 'length' : v[1]} for k, v in gc_trimmed_mean.items()]\n",
    "# Convert the list of dictionaries to dataframes\n",
    "df1 = pd.DataFrame(data)\n",
    "trimmed_df = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "trimmed_df = trimmed_df[trimmed_df['length'] >= plot_min_length]\n",
    "#print(trimmed_df) \n",
    "\n",
    "#add column if contained in keys\n",
    "trimmed_df['in_fasta'] = trimmed_df['id'].isin(tot)\n",
    "#print(trimmed_df)\n",
    "print('Contigs found in this set also in Blast' + str(trimmed_df.in_fasta.sum()))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "#labels = list(trimmed_df['id'])\n",
    "sc1 = plt.scatter(trimmed_df['average_coverage'], trimmed_df['gc_content'], c = trimmed_df['in_fasta'])\n",
    "cbar = plt.colorbar(sc1)\n",
    "# Add labels and legend\n",
    "plt.xlabel('Trimmed Average Coverage %')\n",
    "plt.ylabel('GC content %')\n",
    "plt.title('Blob plot with trimmed averages with chunks of ' + str(chunk_size))\n",
    "ax.legend([sc1], ['in_fasta'], loc=\"upper right\")\n",
    "\n",
    "# save the plot\n",
    "save_path = os.path.join(out_dir, \"Fasta_Blob.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0742f",
   "metadata": {},
   "source": [
    "## Plot histogram of lengths of this subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot of lengths of in fasta in trimmed_df\n",
    "data = trimmed_df[trimmed_df[\"in_fasta\"] == True]\n",
    "data = data.sort_values(by=['length'], ascending=False)\n",
    "#x is ids\n",
    "x = data['id']\n",
    "#y is length of ids\n",
    "y = data['length']\n",
    "\n",
    "#color contigs up to total length of genome size\n",
    "current = 0\n",
    "length = genome_size\n",
    "#add lengths together till greater than length\n",
    "for i, l in enumerate(y):\n",
    "    current += l\n",
    "    if current > length:\n",
    "        #print(i)\n",
    "        cutoff = l\n",
    "        break\n",
    "        \n",
    "mask1 = y >= cutoff\n",
    "mask2 = y < cutoff\n",
    "\n",
    "plt.bar(x[mask1], y[mask1], color = 'red')\n",
    "plt.bar(x[mask2], y[mask2], color = 'blue')\n",
    "#plt.bar(x, y)\n",
    "\n",
    "plt.xlabel('Contig ids')\n",
    "plt.ylabel('Lengths')\n",
    "plt.title('Bar plot of contig lengths in Blast results')\n",
    "# save the plot\n",
    "save_path = os.path.join(out_dir, \"Histogram_of_lengths_of_subset.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391bcae",
   "metadata": {},
   "source": [
    "# Manually inspect and set cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save selected contigs\n",
    "results_cutoff = 190\n",
    "subset = data.head(results_cutoff)\n",
    "#print(subset)\n",
    "subset.to_csv('./out_dir/blast_subset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f27c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each id in blast_subset add that contig to a fasta file\n",
    "ids = []\n",
    "with open('./out_dir/blast_subset.csv','r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        ids.append(row[0])\n",
    "    \n",
    "output_file = open('./data/ourGenome.fasta', 'a')\n",
    "with open(assembly, 'r') as fasta_file:\n",
    "    header = ''\n",
    "    sequence = ''\n",
    "    for line in fasta_file:\n",
    "        if line.startswith('>'):\n",
    "            #check if id in list\n",
    "            if any(id in line for id in ids):\n",
    "                header = line.strip()\n",
    "        else:\n",
    "            sequence = line.strip()\n",
    "            if header and sequence:\n",
    "                output_file.write(header + '\\n')\n",
    "                output_file.write(sequence + '\\n')\n",
    "                header = ''\n",
    "                sequence = ''\n",
    "                \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO find lengths of discarded\n",
    "data = [{'id': k, 'gc_content': v[0], 'length' : v[1]} for k, v in gc_trimmed_mean.items()]\n",
    "# Convert the list of dictionaries to dataframes\n",
    "df1 = pd.DataFrame(data)\n",
    "discarded = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "discarded = discarded[discarded['length'] >= plot_min_length]\n",
    "discarded['in_fasta'] = discarded['id'].isin(tot)\n",
    "#print(discarded)\n",
    "\n",
    "#take only false\n",
    "data = discarded[discarded[\"in_fasta\"] == False]\n",
    "data = data.sort_values(by=['length'], ascending=False)\n",
    "#print(data)\n",
    "\n",
    "#x is ids\n",
    "x = data['id']\n",
    "#y is length of ids\n",
    "y = data['length']\n",
    "plt.bar(x,y)\n",
    "\n",
    "plt.xlabel('Contig ids')\n",
    "plt.ylabel('Lengths')\n",
    "plt.title('Bar plot of contig lengths NOT in Blast results')\n",
    "save_path = os.path.join(out_dir, \"Histogram_of_lengths_not_in_Blast_results.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9cae43",
   "metadata": {},
   "source": [
    "# Now go on to use the Filtered Results :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cladorhizid",
   "language": "python",
   "name": "cladorhizid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
