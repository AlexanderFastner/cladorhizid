{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b318748",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "raw",
   "id": "449dfc15",
   "metadata": {},
   "source": [
    "1. Get GC content from contigs and in chunk sizes, then visualize both\n",
    "2. Get coverage for contigs and in chunk sizes, then visualize both \n",
    "3. Histogram of contig lengths\n",
    "4. Blob plot (gc vs average coverage) for both\n",
    "5. Kmeans clustering\n",
    "6. Take cluster from kmeans as input to Blast_script\n",
    "7. get concencus classification for each contig\n",
    "8. Blob plot again and mark points that had accurate blast hits\n",
    "9. Histogram for length distribution\n",
    "10. Take contigs starting with largest up til genome size\n",
    "11. Add contig ids to fasta file for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7684a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import multiprocess\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2893ee",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0ec1c42",
   "metadata": {},
   "source": [
    "#PARAMETERS\n",
    "# assembly : The fasta input assembly file\n",
    "# coverage_per_contig : A tsv file with information for each contig\n",
    "# coverage_per_base:  A tsv file with coverage information for each base pair position\n",
    "# n_cores: number of cpu cores to multiprocess with\n",
    "# out_dir : output directory\n",
    "# min_contig_size : The minimum size of a contig, otherwise its ignored\n",
    "# chunk_size : The chunk size for dividing up longer contigs (default 10000)\n",
    "# visualize_gc : An option for creating plots showing the distribution of GC content accross a contig in steps of a given chunk size (default = True)\n",
    "# visualize_coverage : An option to create plots showing coverage distribution per contig\n",
    "# visualize_histogram : Option to cisualize histogram of conitg lengths\n",
    "# threshold_pct : The percentage away from the mean that something needs to be in order to be highlighted\n",
    "# visualize_blob : Option to visualize blob plot\n",
    "# plot_min_length : The minimum contig length to be plotted in the Blob plot\n",
    "# n_clusters : Number of clusters for kmeans\n",
    "# len_to_blast : Length of the to be blasted subsections\n",
    "# space_to_blast : Distance between subsections to be blasted\n",
    "# threshold for what % of classification is enough to call that contig\n",
    "# genome_size : Used to take longest contigs up to genome size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1180c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembly = \"./data/cladorhizid_v0.6_hapA/clado_v0.6_hapA.fasta\"\n",
    "coverage_per_contig = \"./data/cladorhizid_hifi_assembly.coverage\"\n",
    "coverage_per_base = \"./data/cladorhizid_hifi_assembly.depth\"\n",
    "n_cores = 2\n",
    "out_dir = \"out_dir/\"\n",
    "min_contig_size = 1000000\n",
    "chunk_size = 10000\n",
    "visualize_gc = True\n",
    "visualize_coverage = True\n",
    "visualize_histogram = True\n",
    "threshold_pct = 50\n",
    "visualize_blob = True\n",
    "plot_min_length = 100000\n",
    "n_clusters = 2\n",
    "len_to_blast = 1000\n",
    "space_to_blast = 10000\n",
    "threshold = 0.8\n",
    "genome_size = 430000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2a78d",
   "metadata": {},
   "source": [
    "# Create outdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a593f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info = out_dir + \"cluster_info/\"\n",
    "coverage_output = out_dir + \"Coverage_output/\"\n",
    "GC_visual_output = out_dir + \"GC_visual_output/\"\n",
    "if not os.path.exists(cluster_info):\n",
    "            os.makedirs(cluster_info)\n",
    "if not os.path.exists(coverage_output):\n",
    "            os.makedirs(coverage_output)\n",
    "if not os.path.exists(GC_visual_output):\n",
    "            os.makedirs(GC_visual_output)\n",
    "cluster_info = ''\n",
    "coverage_output = ''\n",
    "GC_visual_output = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b434043",
   "metadata": {},
   "source": [
    "# Naive GC content for each content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff775cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content(fasta_file):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to their GC content\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq y min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:\n",
    "                        gc_content[current_id] = [(current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq), len(current_seq)]\n",
    "                        \n",
    "                # Start the new contig\n",
    "                #current_id = line.strip()[1:].split(\"/\")[3]\n",
    "                current_id = line.strip().strip('>')\n",
    "                current_seq = \"\"\n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "    \n",
    "    # Calculate GC content for the final contig\n",
    "    if len(current_seq) > min_contig_size:\n",
    "        gc_content[current_id] = [(current_seq.count(\"G\") + current_seq.count(\"C\")) / len(current_seq), len(current_seq)]\n",
    "    \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c675f328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "naive_gc = get_gc_content(assembly)\n",
    "print(len(naive_gc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad90d78",
   "metadata": {},
   "source": [
    "# GC content for each contig by averaging in given chunk sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad03fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gc_content_chunks(fasta_file, chunk_size):\n",
    "    \"\"\"\n",
    "    Reads in a FASTA file and returns a dictionary mapping contig IDs to a list of gc content per chunk size\n",
    "    \"\"\"\n",
    "    gc_content = {}\n",
    "    current_id = None\n",
    "    current_seq = \"\"\n",
    "    chunks = []\n",
    "    \n",
    "    with open(fasta_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\">\"):\n",
    "                # If this is a new contig, calculate the GC content for the previous one (if there was one)\n",
    "                if current_id is not None:\n",
    "                    #remove if current_seq < min_contig_size\n",
    "                    if len(current_seq) > min_contig_size:  \n",
    "                        i = 0\n",
    "                        while i < len(current_seq):\n",
    "                            # check for last chunk\n",
    "                            if (i + chunk_size) > len(current_seq):\n",
    "                                end = len(current_seq)\n",
    "                            else:\n",
    "                                end = i + chunk_size\n",
    "                            sub = current_seq[i:end]\n",
    "                            gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)),2)\n",
    "                            chunks.append(gc)\n",
    "                            i = end\n",
    "                        # add to dictionary in form {current_id : [array of GC content of given chunk size]}\n",
    "                        gc_content[current_id] = [chunks, len(current_seq)]\n",
    "\n",
    "                # Start the new contig\n",
    "                #current_id = line.strip()[1:].split(\"/\")[3]\n",
    "                current_id = line.strip()\n",
    "                current_seq = \"\"\n",
    "                chunks = []\n",
    "                \n",
    "            else:\n",
    "                current_seq += line.strip()\n",
    "                \n",
    "        # calculate GC content for the last contig\n",
    "        if current_id is not None:\n",
    "            i = 0\n",
    "            while i < len(current_seq):\n",
    "                # check for last chunk\n",
    "                if (i + chunk_size) > len(current_seq):\n",
    "                    end = len(current_seq)\n",
    "                else:\n",
    "                    end = i + chunk_size\n",
    "                sub = current_seq[i:end]\n",
    "                gc = round(((sub.count(\"G\") + sub.count(\"C\")) / len(sub)), 2)\n",
    "                chunks.append(gc)\n",
    "                i = end\n",
    "            # add to dictionary in form {current_id : [array of GC content of given chunk size, length]}\n",
    "            gc_content[current_id] = [chunks, len(current_seq)]\n",
    "        \n",
    "    return gc_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d18eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length check:  41\n"
     ]
    }
   ],
   "source": [
    "chunks = get_gc_content_chunks(assembly, chunk_size)\n",
    "gc_trimmed_mean = {}\n",
    "#Get the trimmed mean from chunks\n",
    "for entry in chunks:\n",
    "    length = chunks.get(entry)[1]\n",
    "    avg_cov = chunks.get(entry)[0]\n",
    "    gc_trimmed_mean[entry] = [stats.trim_mean(avg_cov, 0.05), length]\n",
    "print('Length check: ', len(gc_trimmed_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e2afd",
   "metadata": {},
   "source": [
    "# Visualize GC content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b706d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping >CLA1_hapA (already exists)Skipping >CLA7_hapA (already exists)\n",
      "\n",
      "Skipping >CLA2_hapA (already exists)Skipping >CLA8_hapA (already exists)\n",
      "\n",
      "Skipping >CLA3_hapA (already exists)Skipping >CLA9_hapA (already exists)\n",
      "\n",
      "Skipping >CLA4_hapA (already exists)\n",
      "Skipping >CLA10_hapA (already exists)\n",
      "Skipping >CLA5_hapA (already exists)\n",
      "Skipping >CLA11_hapA (already exists)\n",
      "Skipping >CLA6_hapA (already exists)\n",
      "Skipping >CLA12_hapA (already exists)\n",
      "Skipping >CLA13_hapA (already exists)\n",
      "Skipping >CLA_hapA_sca1 (already exists)\n",
      "Skipping >CLA14_hapA (already exists)\n",
      "Skipping >CLA_hapA_sca2 (already exists)\n",
      "Skipping >CLA15_hapA (already exists)\n",
      "Skipping >CLA_hapA_sca3 (already exists)\n",
      "Skipping >CLA16_hapA (already exists)\n",
      "Skipping >CLA_hapA_sca4 (already exists)\n",
      "Skipping >CLA17_hapA (already exists)\n",
      "Skipping >CLA_hapA_sca5 (already exists)\n",
      "Skipping >CLA18_hapA (already exists)\n",
      "Skipping >CLA_hapA_sca6 (already exists)\n",
      "Skipping >CLA_hapA_sca7 (already exists)\n",
      "Skipping >CLA_hapA_sca13 (already exists)\n",
      "Skipping >CLA_hapA_sca8 (already exists)\n",
      "Skipping >CLA_hapA_sca14 (already exists)\n",
      "Skipping >CLA_hapA_sca9 (already exists)\n",
      "Skipping >CLA_hapA_sca15 (already exists)\n",
      "Skipping >CLA_hapA_sca10 (already exists)\n",
      "Skipping >CLA_hapA_sca16 (already exists)\n",
      "Skipping >CLA_hapA_sca11 (already exists)\n",
      "Skipping >CLA_hapA_sca17 (already exists)\n",
      "Skipping >CLA_hapA_sca12 (already exists)\n",
      "Skipping >CLA_hapA_sca18 (already exists)\n",
      "Skipping >CLA_hapA_sca19 (already exists)\n",
      "Skipping >CLA_hapA_sca20 (already exists)\n",
      "Skipping >CLA_hapA_sca21 (already exists)\n",
      "Skipping >CLA_hapA_sca22 (already exists)\n",
      "Skipping >CLA_hapA_sca1032 (already exists)\n",
      "Starting Visualize GC content\n",
      "Finished Visualize GC content\n"
     ]
    }
   ],
   "source": [
    "if visualize_gc:\n",
    "\n",
    "    save_dir = out_dir + \"/GC_visual_output\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    def process_gc_plot(chunk):\n",
    "        id = chunk\n",
    "        \n",
    "        # added to resume if crashes for whatever reason\n",
    "        save_path = os.path.join(save_dir, \"GC_content_\" + str(id) + \".png\")\n",
    "        \n",
    "        # Skip if the plot already exists\n",
    "        if not os.path.exists(save_path):\n",
    "            gc_content = np.array(chunks.get(chunk)[0])\n",
    "\n",
    "            # create a bar plot\n",
    "            fig, ax = plt.subplots()\n",
    "            mean_gc = np.mean(gc_content)\n",
    "            colors = ['red' if abs(gc - mean_gc) / mean_gc * 100 >= threshold_pct else 'blue' for gc in gc_content]\n",
    "            plt.bar(range(len(gc_content)), gc_content, color=colors, width=1)\n",
    "            ax.axhline(y=gc_trimmed_mean[id][0], color='black')\n",
    "            plt.title(\"Contig: \" + str(id))\n",
    "            plt.xlabel('index of Chunk of size: ' + str(chunk_size))\n",
    "            plt.ylabel('GC content')\n",
    "            #save the plot\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            print(f'Skipping {id} (already exists)')\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        with multiprocess.Pool(processes=n_cores) as pool:\n",
    "            print('Starting Visualize GC content')\n",
    "            pool.map(process_gc_plot, chunks.keys())\n",
    "            print('Finished Visualize GC content')\n",
    "\n",
    "else:\n",
    "    print(\"GC visualization is turned off.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a8705",
   "metadata": {},
   "source": [
    "# Naive coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9a58bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig(coverage_file):\n",
    "    naive_coverage = {}\n",
    "\n",
    "    with open(coverage_file, \"r\") as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            # Split line into columns\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "            #print(columns)\n",
    "\n",
    "            #only add if length > min_contig_size\n",
    "            if (int(columns[2]) - int(columns[1])) > min_contig_size:\n",
    "                # Use the first column as key and the sixth column as value\n",
    "                key = columns[0]\n",
    "                value = columns[6]\n",
    "                length = int(columns[2]) - int(columns[1])\n",
    "\n",
    "                # Add key-value pair to dictionary\n",
    "                naive_coverage[key] = [float(value), length]\n",
    "            \n",
    "    f.close()\n",
    "    return naive_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92cdd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "naive_coverage = get_coverage_per_contig(coverage_per_contig)\n",
    "print(len(naive_coverage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb8a1c",
   "metadata": {},
   "source": [
    "# Chunk averaged Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a896c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage_per_contig_by_chunks(depth_file, chunk_size):\n",
    "    save_dir = out_dir + \"Coverage_output/\"\n",
    "    \n",
    "    # Define a dictionary to store the coverage data for each ID\n",
    "    id_data = {}\n",
    "\n",
    "    # Read in the TSV file\n",
    "    with open(depth_file, 'r') as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "\n",
    "        # Iterate over each row in the file\n",
    "        for i, row in enumerate(reader):\n",
    "            if i % 100000000 == 0:\n",
    "                print(f'reading in line {i}')\n",
    "                \n",
    "            # Extract the ID, index, and coverage from the row\n",
    "            id = row[0]\n",
    "            index = int(row[1])\n",
    "            coverage = float(row[2])\n",
    "\n",
    "            # Check if this is the first row for this ID\n",
    "            if id not in id_data:\n",
    "                id_data[id] = []\n",
    "\n",
    "            # Find the window index for this row\n",
    "            window_index = index // chunk_size\n",
    "\n",
    "            # Check if there is already coverage data for this window\n",
    "            if len(id_data[id]) <= window_index:\n",
    "                # Add a new entry for this window\n",
    "                id_data[id].append({'window_sum': coverage, 'window_count': 1})\n",
    "            else:\n",
    "                # Add the coverage to the existing window data\n",
    "                id_data[id][window_index]['window_sum'] += coverage\n",
    "                id_data[id][window_index]['window_count'] += 1    \n",
    "    #id_data is a dictionary of ids, that have an array of {'window_sum': coverage, 'window_count': 1}\n",
    "    #id_data = [{'window_sum': coverage, 'window_count': 1}] summed total of coverage column, total number of windows           \n",
    "    \n",
    "    # Write the output to a file\n",
    "    # We want one line for each window (10kb), and the average coverage for that window for each Id\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", 'w') as outfile:\n",
    "        outfile.write(\"id\\tindex\\taverage_coverage\\n\")\n",
    "        for id, windows in id_data.items():\n",
    "            #check total length > min_contig_size\n",
    "            total_len = 0\n",
    "            for i, window in enumerate(windows):\n",
    "                total_len += window['window_count']\n",
    "            if total_len > min_contig_size:\n",
    "                for i, window in enumerate(windows):\n",
    "                    if window['window_count'] > 0:\n",
    "                        avg_coverage = round((window['window_sum'] / window['window_count']), 2)\n",
    "                        outfile.write(f\"{id}\\t{i}\\t{avg_coverage}\\n\")\n",
    "                        \n",
    "        outfile.close()\n",
    "    \n",
    "    # now get the average coverage for each id\n",
    "    # Open the input file\n",
    "    with open(save_dir + \"chunked_windows_coverage.tsv\", \"r\") as f:\n",
    "        next(f)\n",
    "        # Create a dictionary to store the total coverage and highest index for each id\n",
    "        id_coverage = {}\n",
    "        # Read through each line in the file\n",
    "        for line in f:\n",
    "            # Split the line into three columns\n",
    "            id, index, coverage = line.strip().split(\"\\t\")\n",
    "            # Convert the coverage to float\n",
    "            index = float(index)\n",
    "            coverage = float(coverage)\n",
    "            # If the id is not in the dictionary yet, add it with a coverage of 0 and index of -1\n",
    "            if id not in id_coverage:\n",
    "                id_coverage[id] = {\"coverage\": 0, \"max_index\": -1}\n",
    "            # Add the coverage to the total for this id\n",
    "            id_coverage[id][\"coverage\"] += coverage\n",
    "            # Update the max index for this id if the current index is higher\n",
    "            if index > id_coverage[id][\"max_index\"]:\n",
    "                id_coverage[id][\"max_index\"] = index\n",
    "    f.close()\n",
    "    \n",
    "    #make list of coverages, then take trimmed mean of that list\n",
    "    coverage_trimmed_mean = {}\n",
    "    for entry in id_data:\n",
    "        coverages = []\n",
    "        for chunk in id_data.get(entry):\n",
    "            window_sum = float(chunk.get('window_sum'))\n",
    "            window_count = float(chunk.get('window_count'))\n",
    "            coverage = round((window_sum / window_count), 2)\n",
    "            coverages.append(coverage)\n",
    "        \n",
    "        coverage_trimmed_mean[entry] = round(stats.trim_mean(coverages, 0.05), 2)\n",
    "        \n",
    "    # Open the output file\n",
    "    with open(save_dir + \"chunked_coverage.tsv\", \"w\") as f:\n",
    "        # Write the header row\n",
    "        f.write(\"id\\taverage_coverage\\n\")\n",
    "        # Loop through the ids in the dictionary\n",
    "        for id in coverage_trimmed_mean:\n",
    "            average_coverage = coverage_trimmed_mean[id]\n",
    "            f.write(\"{}\\t{}\\n\".format(id, average_coverage))\n",
    "    f.close()\n",
    "    \n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fb693cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in line 0\n",
      "reading in line 100000000\n",
      "reading in line 200000000\n",
      "reading in line 300000000\n",
      "reading in line 400000000\n",
      "reading in line 500000000\n",
      "reading in line 600000000\n",
      "reading in line 700000000\n",
      "reading in line 800000000\n",
      "reading in line 900000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1026"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_coverage = get_coverage_per_contig_by_chunks(coverage_per_base, chunk_size)\n",
    "len(chunks_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0121ae3a",
   "metadata": {},
   "source": [
    "# Visualize Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474b97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Visualize Coverage\n",
      "loaded data\n",
      "loaded mean\n",
      "Length of unique ids:  40\n",
      "CLA1_hapA\n",
      "start CLA1_hapA\n",
      "             id  index  average_coverage\n",
      "0     CLA1_hapA      0             50.76\n",
      "1     CLA1_hapA      1             49.98\n",
      "2     CLA1_hapA      2             87.66\n",
      "3     CLA1_hapA      3             85.66\n",
      "4     CLA1_hapA      4             52.07\n",
      "...         ...    ...               ...\n",
      "3584  CLA1_hapA   3584             55.57\n",
      "3585  CLA1_hapA   3585             62.90\n",
      "3586  CLA1_hapA   3586             58.82\n",
      "3587  CLA1_hapA   3587            121.67\n",
      "3588  CLA1_hapA   3588            150.31\n",
      "\n",
      "[3589 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  47.68\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA2_hapA\n",
      "start CLA2_hapA\n",
      "             id  index  average_coverage\n",
      "3589  CLA2_hapA      0             84.26\n",
      "3590  CLA2_hapA      1             37.11\n",
      "3591  CLA2_hapA      2             83.18\n",
      "3592  CLA2_hapA      3             79.85\n",
      "3593  CLA2_hapA      4             93.33\n",
      "...         ...    ...               ...\n",
      "7326  CLA2_hapA   3737            103.39\n",
      "7327  CLA2_hapA   3738             81.08\n",
      "7328  CLA2_hapA   3739            100.24\n",
      "7329  CLA2_hapA   3740            106.90\n",
      "7330  CLA2_hapA   3741             37.52\n",
      "\n",
      "[3742 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  46.66\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA3_hapA\n",
      "start CLA3_hapA\n",
      "              id  index  average_coverage\n",
      "7331   CLA3_hapA      0             65.81\n",
      "7332   CLA3_hapA      1             62.35\n",
      "7333   CLA3_hapA      2             68.00\n",
      "7334   CLA3_hapA      3            159.99\n",
      "7335   CLA3_hapA      4             10.69\n",
      "...          ...    ...               ...\n",
      "11505  CLA3_hapA   4174             41.59\n",
      "11506  CLA3_hapA   4175             50.72\n",
      "11507  CLA3_hapA   4176             31.63\n",
      "11508  CLA3_hapA   4177             38.24\n",
      "11509  CLA3_hapA   4178             26.83\n",
      "\n",
      "[4179 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  49.12\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA4_hapA\n",
      "start CLA4_hapA\n",
      "              id  index  average_coverage\n",
      "11510  CLA4_hapA      0             10.45\n",
      "11511  CLA4_hapA      1             24.20\n",
      "11512  CLA4_hapA      2             37.27\n",
      "11513  CLA4_hapA      3             23.73\n",
      "11514  CLA4_hapA      4             16.94\n",
      "...          ...    ...               ...\n",
      "16475  CLA4_hapA   4965             87.26\n",
      "16476  CLA4_hapA   4966             28.57\n",
      "16477  CLA4_hapA   4967             24.89\n",
      "16478  CLA4_hapA   4968             51.69\n",
      "16479  CLA4_hapA   4969             52.44\n",
      "\n",
      "[4970 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  45.36\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA5_hapA\n",
      "start CLA5_hapA\n",
      "              id  index  average_coverage\n",
      "16480  CLA5_hapA      0             31.80\n",
      "16481  CLA5_hapA      1             29.96\n",
      "16482  CLA5_hapA      2             54.87\n",
      "16483  CLA5_hapA      3             54.45\n",
      "16484  CLA5_hapA      4             65.22\n",
      "...          ...    ...               ...\n",
      "20950  CLA5_hapA   4470             40.91\n",
      "20951  CLA5_hapA   4471             60.14\n",
      "20952  CLA5_hapA   4472             50.66\n",
      "20953  CLA5_hapA   4473             55.65\n",
      "20954  CLA5_hapA   4474             54.01\n",
      "\n",
      "[4475 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  48.12\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA6_hapA\n",
      "start CLA6_hapA\n",
      "              id  index  average_coverage\n",
      "20955  CLA6_hapA      0             11.15\n",
      "20956  CLA6_hapA      1             32.44\n",
      "20957  CLA6_hapA      2             27.53\n",
      "20958  CLA6_hapA      3             17.35\n",
      "20959  CLA6_hapA      4             22.64\n",
      "...          ...    ...               ...\n",
      "26018  CLA6_hapA   5063             35.96\n",
      "26019  CLA6_hapA   5064             30.39\n",
      "26020  CLA6_hapA   5065             35.49\n",
      "26021  CLA6_hapA   5066             45.55\n",
      "26022  CLA6_hapA   5067             62.15\n",
      "\n",
      "[5068 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  49.23\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA7_hapA\n",
      "start CLA7_hapA\n",
      "              id  index  average_coverage\n",
      "26023  CLA7_hapA      0             34.05\n",
      "26024  CLA7_hapA      1             69.77\n",
      "26025  CLA7_hapA      2             59.73\n",
      "26026  CLA7_hapA      3             34.81\n",
      "26027  CLA7_hapA      4             37.62\n",
      "...          ...    ...               ...\n",
      "30646  CLA7_hapA   4623              3.64\n",
      "30647  CLA7_hapA   4624              2.67\n",
      "30648  CLA7_hapA   4625              7.45\n",
      "30649  CLA7_hapA   4626             19.40\n",
      "30650  CLA7_hapA   4627             23.45\n",
      "\n",
      "[4628 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  50.36\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA8_hapA\n",
      "start CLA8_hapA\n",
      "              id  index  average_coverage\n",
      "30651  CLA8_hapA      0             47.01\n",
      "30652  CLA8_hapA      1             88.02\n",
      "30653  CLA8_hapA      2             71.73\n",
      "30654  CLA8_hapA      3             61.00\n",
      "30655  CLA8_hapA      4             63.63\n",
      "...          ...    ...               ...\n",
      "35633  CLA8_hapA   4982             32.16\n",
      "35634  CLA8_hapA   4983             31.95\n",
      "35635  CLA8_hapA   4984             71.11\n",
      "35636  CLA8_hapA   4985             58.43\n",
      "35637  CLA8_hapA   4986             15.36\n",
      "\n",
      "[4987 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  44.97\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA9_hapA\n",
      "start CLA9_hapA\n",
      "              id  index  average_coverage\n",
      "35638  CLA9_hapA      0             13.65\n",
      "35639  CLA9_hapA      1             62.89\n",
      "35640  CLA9_hapA      2             91.56\n",
      "35641  CLA9_hapA      3            153.42\n",
      "35642  CLA9_hapA      4             52.02\n",
      "...          ...    ...               ...\n",
      "39988  CLA9_hapA   4350             27.23\n",
      "39989  CLA9_hapA   4351             39.69\n",
      "39990  CLA9_hapA   4352             48.91\n",
      "39991  CLA9_hapA   4353             78.73\n",
      "39992  CLA9_hapA   4354            327.92\n",
      "\n",
      "[4355 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  47.0\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA10_hapA\n",
      "start CLA10_hapA\n",
      "               id  index  average_coverage\n",
      "39993  CLA10_hapA      0             44.97\n",
      "39994  CLA10_hapA      1            204.58\n",
      "39995  CLA10_hapA      2             92.06\n",
      "39996  CLA10_hapA      3             44.09\n",
      "39997  CLA10_hapA      4             33.20\n",
      "...           ...    ...               ...\n",
      "43854  CLA10_hapA   3861             61.91\n",
      "43855  CLA10_hapA   3862            156.79\n",
      "43856  CLA10_hapA   3863             69.46\n",
      "43857  CLA10_hapA   3864             95.39\n",
      "43858  CLA10_hapA   3865             17.93\n",
      "\n",
      "[3866 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  46.42\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA11_hapA\n",
      "start CLA11_hapA\n",
      "               id  index  average_coverage\n",
      "43859  CLA11_hapA      0             88.76\n",
      "43860  CLA11_hapA      1             64.01\n",
      "43861  CLA11_hapA      2             69.48\n",
      "43862  CLA11_hapA      3             74.67\n",
      "43863  CLA11_hapA      4             58.92\n",
      "...           ...    ...               ...\n",
      "49414  CLA11_hapA   5555             38.67\n",
      "49415  CLA11_hapA   5556             49.82\n",
      "49416  CLA11_hapA   5557             37.08\n",
      "49417  CLA11_hapA   5558             58.57\n",
      "49418  CLA11_hapA   5559            136.87\n",
      "\n",
      "[5560 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  47.48\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA12_hapA\n",
      "start CLA12_hapA\n",
      "               id  index  average_coverage\n",
      "49419  CLA12_hapA      0             21.26\n",
      "49420  CLA12_hapA      1             23.03\n",
      "49421  CLA12_hapA      2             22.50\n",
      "49422  CLA12_hapA      3             56.61\n",
      "49423  CLA12_hapA      4             36.92\n",
      "...           ...    ...               ...\n",
      "53080  CLA12_hapA   3661             87.06\n",
      "53081  CLA12_hapA   3662             49.27\n",
      "53082  CLA12_hapA   3663             63.51\n",
      "53083  CLA12_hapA   3664             54.31\n",
      "53084  CLA12_hapA   3665             31.00\n",
      "\n",
      "[3666 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  47.61\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA13_hapA\n",
      "start CLA13_hapA\n",
      "               id  index  average_coverage\n",
      "53085  CLA13_hapA      0             36.75\n",
      "53086  CLA13_hapA      1             63.97\n",
      "53087  CLA13_hapA      2             81.04\n",
      "53088  CLA13_hapA      3             52.64\n",
      "53089  CLA13_hapA      4             61.34\n",
      "...           ...    ...               ...\n",
      "57449  CLA13_hapA   4364             35.83\n",
      "57450  CLA13_hapA   4365             25.63\n",
      "57451  CLA13_hapA   4366             29.86\n",
      "57452  CLA13_hapA   4367             16.13\n",
      "57453  CLA13_hapA   4368              4.69\n",
      "\n",
      "[4369 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  43.44\n",
      "changed colors\n",
      "plotted\n",
      "saved\n",
      "CLA14_hapA\n",
      "start CLA14_hapA\n",
      "               id  index  average_coverage\n",
      "57454  CLA14_hapA      0             49.05\n",
      "57455  CLA14_hapA      1             38.54\n",
      "57456  CLA14_hapA      2             28.66\n",
      "57457  CLA14_hapA      3             25.45\n",
      "57458  CLA14_hapA      4             22.41\n",
      "...           ...    ...               ...\n",
      "60922  CLA14_hapA   3468             38.74\n",
      "60923  CLA14_hapA   3469             71.55\n",
      "60924  CLA14_hapA   3470             58.97\n",
      "60925  CLA14_hapA   3471             43.75\n",
      "60926  CLA14_hapA   3472             39.00\n",
      "\n",
      "[3473 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  45.1\n",
      "changed colors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotted\n",
      "saved\n",
      "CLA15_hapA\n",
      "start CLA15_hapA\n",
      "               id  index  average_coverage\n",
      "60927  CLA15_hapA      0             50.50\n",
      "60928  CLA15_hapA      1             45.57\n",
      "60929  CLA15_hapA      2             48.95\n",
      "60930  CLA15_hapA      3             29.35\n",
      "60931  CLA15_hapA      4             74.74\n",
      "...           ...    ...               ...\n",
      "65148  CLA15_hapA   4221             44.74\n",
      "65149  CLA15_hapA   4222             77.51\n",
      "65150  CLA15_hapA   4223             57.96\n",
      "65151  CLA15_hapA   4224             28.25\n",
      "65152  CLA15_hapA   4225              8.16\n",
      "\n",
      "[4226 rows x 3 columns]\n",
      "got trimmed_mean_coverage:  47.22\n",
      "changed colors\n",
      "plotted\n"
     ]
    }
   ],
   "source": [
    "# Warning, for long pacbio reads this uses a LOT of RAM, if your system has less than 16GB beware!\n",
    "\n",
    "# memory_info = psutil.Process().memory_info()\n",
    "# print(f\"Current Memory used: {memory_info.rss / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "if visualize_coverage:\n",
    "    print('Starting Visualize Coverage')\n",
    "    save_dir = out_dir + \"Coverage_output/\"\n",
    "    df = pd.DataFrame(pd.read_csv(save_dir + \"/chunked_windows_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\"))\n",
    "    print('loaded data')\n",
    "    mean = pd.DataFrame(pd.read_csv(save_dir + \"/chunked_coverage.tsv\", dtype={'id': 'object'}, sep = \"\\t\"))\n",
    "    print('loaded mean')\n",
    "    \n",
    "    # Get a list of unique ids\n",
    "    unique_ids = df['id'].unique()\n",
    "    print('Length of unique ids: ', len(unique_ids))\n",
    "    \n",
    "    #Plotting\n",
    "    #--------------- \n",
    "    fig, ax = plt.subplots()\n",
    "    plt.xlabel('Index of chunk size ' + str(chunk_size))\n",
    "    plt.ylabel('Average coverage')\n",
    "    gc.enable()\n",
    "    \n",
    "    for id in unique_ids:\n",
    "        print(id)\n",
    "        \n",
    "        # added to resume if crashes for whatever reason\n",
    "        save_path = os.path.join(save_dir, \"Coverage_\" + str(id) + \".png\")\n",
    "        \n",
    "        # Skip if the plot already exists\n",
    "        if os.path.exists(save_path):\n",
    "            print(f'Skipping {id} (already exists)')\n",
    "            continue\n",
    "            \n",
    "        # Filter the dataframe to get only the rows for this id\n",
    "        search_str = f\"id == '{id}'\"\n",
    "        #print(search_str)\n",
    "        df_id = df.query(search_str)\n",
    "        trimmed_mean_coverage = float(mean.loc[mean['id'] == id, 'average_coverage'])\n",
    "        ax.axhline(y=trimmed_mean_coverage, color='black')\n",
    "        # filtered colors\n",
    "        colors = ['red' if abs(coverage - trimmed_mean_coverage) / trimmed_mean_coverage * 100 >= threshold_pct else 'blue' for coverage in df_id['average_coverage']]\n",
    "        plt.bar(df_id['index'], df_id['average_coverage'], label=f\"id={id}\", color=colors, width=1)\n",
    "        plt.savefig(save_path)\n",
    "        plt.clf()\n",
    "        \n",
    "    print('Finished Visualize Coverage')\n",
    "\n",
    "else :\n",
    "    print(\"Coverage visualization is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d53fe",
   "metadata": {},
   "source": [
    "# Blob Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot gc content (x) vs coverage (y)\n",
    "if visualize_blob:\n",
    "\n",
    "    data = [{'id': k, 'gc_content': v[0]} for k, v in naive_gc.items()]\n",
    "    data2 = [{'id': k, 'average_coverage': v[0], 'length': v[1]} for k, v in naive_coverage.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    df2 = pd.DataFrame(data2)\n",
    "    naive_df = pd.merge(df1, df2, on = 'id')\n",
    "    naive_df = naive_df[naive_df['length'] >= plot_min_length]\n",
    "    #print(naive_df)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([0, 100])\n",
    "    cmap = plt.matplotlib.colormaps.get_cmap('plasma')\n",
    "\n",
    "    sc = plt.scatter(naive_df['average_coverage'], naive_df['gc_content'],  c = naive_df['length'], cmap = cmap)\n",
    "    cbar = plt.colorbar(sc)\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Average Coverage %')\n",
    "    plt.ylabel('GC content %')\n",
    "    plt.title('Blob plot with naive averages')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    ax.legend([sc], ['Length'], loc=\"upper right\")\n",
    "    \n",
    "    # save the plot\n",
    "    save_path = os.path.join(out_dir, \"Blob_naive.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "else :\n",
    "    print(\"Visualize Naive Blob is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42338553",
   "metadata": {},
   "source": [
    "# Visualize trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beac461",
   "metadata": {},
   "outputs": [],
   "source": [
    "if visualize_blob:\n",
    "\n",
    "    data = [{'id': k, 'gc_content': v[0], 'length' : v[1]} for k, v in gc_trimmed_mean.items()]\n",
    "    # Convert the list of dictionaries to dataframes\n",
    "    df1 = pd.DataFrame(data)\n",
    "    trimmed_df = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "    trimmed_df = trimmed_df[trimmed_df['length'] >= plot_min_length]\n",
    "    #print(trimmed_df) \n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim([0, 100])\n",
    "    cmap = plt.matplotlib.colormaps.get_cmap('plasma')\n",
    "\n",
    "    #labels = list(trimmed_df['id'])\n",
    "    sc1 = plt.scatter(trimmed_df['average_coverage'], trimmed_df['gc_content'], c = trimmed_df['length'], cmap = cmap)\n",
    "    cbar = plt.colorbar(sc1)\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Trimmed Average Coverage %')\n",
    "    plt.ylabel('GC content %')\n",
    "    plt.title('Blob plot with trimmed averages with chunks of ' + str(chunk_size))\n",
    "    ax.legend([sc1], ['Length'], loc=\"upper right\")\n",
    "\n",
    "    # save the plot\n",
    "    save_path = os.path.join(out_dir, \"Blob.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "else :\n",
    "    print(\"Visualize Trimmed Blob is turned off.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e549a8",
   "metadata": {},
   "source": [
    "# Histogram of contig lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of all contig lengths\n",
    "\n",
    "if visualize_histogram:\n",
    "    data = trimmed_df[\"length\"]\n",
    "    #print(data)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    plt.hist(data, bins=50, edgecolor='black')\n",
    "    ax.ticklabel_format(style='plain', axis='x')\n",
    "    \n",
    "    plt.xlabel('Contig Lengths')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of contig lengths')\n",
    "    \n",
    "    # save the plot\n",
    "    save_path = os.path.join(out_dir, \"Histogram_of_lengths.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0457296f",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca36761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans to get clusters in blob plots\n",
    "np.random.seed(42)\n",
    "\n",
    "coverage = trimmed_df['average_coverage']\n",
    "gc_content = trimmed_df['gc_content']\n",
    "ids = trimmed_df['id'].values\n",
    "\n",
    "#make a feature matrix\n",
    "X = np.column_stack((coverage, gc_content))\n",
    "\n",
    "#kmeans clusters = n_clusters\n",
    "kmeans = KMeans(n_clusters, random_state = 42, n_init='auto').fit(X)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "#create a dictionary for each cluster\n",
    "cluster_points = {i: [] for i in range(len(centroids))}\n",
    "\n",
    "#add the ids to each cluster in the dictionary\n",
    "for i, label in enumerate(labels):\n",
    "    cluster_points[label].append(ids[i])\n",
    "print('Using this plot you will need to select which cluster is the main cluster for the next step')\n",
    "\n",
    "#look up each point in dataframe to get length\n",
    "for cluster in cluster_points:\n",
    "    total_cluster_length = 0\n",
    "    #for each id for each cluster into a seperate file\n",
    "    with open(out_dir + 'cluster_info/cluster_' + str(cluster) + '.txt', 'w') as out:\n",
    "        for i in cluster_points[cluster]:\n",
    "            #write ids into output\n",
    "            out.write(i + '\\n')\n",
    "            \n",
    "            #get length and total length\n",
    "            length = trimmed_df.loc[trimmed_df['id'] == i, 'length'].iloc[0]\n",
    "            total_cluster_length += length\n",
    "    print('total length of cluster ' + str(cluster) + ' : ' + str(total_cluster_length))\n",
    "\n",
    "    print('Cluster 0: aqua star, Cluster 1: green star')\n",
    "colors = ['aqua', 'lawngreen']\n",
    "#plotting\n",
    "plt.scatter(X[:,0], X[:,1], c = labels)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], marker = '*', s = 100, c = colors)\n",
    "plt.xlabel('Coverage %')\n",
    "plt.ylabel('GC_content %')\n",
    "plt.title('K-means clustering | Coverage vs GC_content')\n",
    "\n",
    "# save the plot\n",
    "save_path = os.path.join(out_dir, \"Kmeans_clustering.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d26540",
   "metadata": {},
   "source": [
    "# Making Fasta with ids in target cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(n_clusters):\n",
    "    #Set target cluster \n",
    "    target_cluster = n\n",
    "    if not os.path.exists(out_dir + 'cluster_info/cluster_' + str(target_cluster)+ '/split_contigs/'):\n",
    "                os.makedirs(out_dir + 'cluster_info/cluster_' + str(target_cluster)+ '/split_contigs/')\n",
    "\n",
    "    #for each id in cluster 0 get the sequence and write into a fasta file\n",
    "        #we already have a fasta file with these ids, we just need to extract only the ones we are interested in\n",
    "    with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '.fasta', 'w') as fasta:\n",
    "        with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '.txt', 'r') as cluster:\n",
    "            for line in cluster:\n",
    "                header = line.strip()\n",
    "                #write header\n",
    "                fasta.write('>' + header + '\\n')\n",
    "                #before next header write line for sequence\n",
    "                #search for the header line in assembly which includes header\n",
    "                with open(assembly, 'r') as big:\n",
    "                    found = False\n",
    "                    for line in big:\n",
    "                        if found:\n",
    "                            sequence = line.strip()\n",
    "                            found = False\n",
    "                            break\n",
    "                        if header in line:\n",
    "                            found = True\n",
    "                big.close()\n",
    "                fasta.write(str(sequence) + '\\n')\n",
    "\n",
    "        cluster.close()\n",
    "    fasta.close()\n",
    "\n",
    "    #Split each entry in fasta file, i.e every 10k take 1k sample\n",
    "    #for each entry in fasta file, if length > 10000 then take small chunk every 10000bp\n",
    "    with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '.fasta', 'r') as fasta:\n",
    "        for line in fasta:\n",
    "            if '>' in line:\n",
    "                #write in split_contigs\n",
    "                #print(line)\n",
    "                line = ''.join(line.split())\n",
    "                contig_name = str(line.replace('>', ''))\n",
    "            else:\n",
    "                with open(out_dir + 'cluster_info/cluster_' + str(target_cluster) + '/split_contigs/' + contig_name + '.fasta', 'w') as contig_file:\n",
    "                    #split into substrings of len_to_blast every space_to_blast\n",
    "                    length = len(line)\n",
    "                    #print(length)\n",
    "                    if length > (space_to_blast + len_to_blast):\n",
    "                        #loop\n",
    "                        pos = 0\n",
    "                        while pos < length:\n",
    "                            start = pos\n",
    "                            end = pos + len_to_blast\n",
    "                            #header = startpos endpos\n",
    "                            #sequence = sequence\n",
    "                            seq = (line[start:end] + '\\n')\n",
    "                            header = ('>' + str(start) + ' : ' + str(end) + '\\n')\n",
    "                            pos += space_to_blast + len_to_blast\n",
    "                            contig_file.write(header)\n",
    "                            contig_file.write(seq)\n",
    "\n",
    "                contig_file.close()\n",
    "\n",
    "    fasta.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feafc09",
   "metadata": {},
   "source": [
    "# STOP! Run Blast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84feccc",
   "metadata": {},
   "source": [
    "# Adjust Cluster 0 and Cluster 1 to where you saved your Blast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a060f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_0 = './100kb_subset_data/cluster_0/split_contigs/'\n",
    "cluster_1 = './100kb_subset_data/cluster_1/split_contigs/'\n",
    "clusters = [cluster_0, cluster_1]\n",
    "needed = []\n",
    "needed1 = []\n",
    "# designate needed to be what you are looking for\n",
    "\n",
    "#dictionaries\n",
    "octocoral = ['Dendronephthya gigantea', 'Xenia sp. Carnegie-2017']\n",
    "dinoflagellates = ['Breviolum minutum Mf 1.05b.01','Symbiodinium microadriaticum','Symbiodinium sp. clade A Y106',\n",
    "                   'Symbiodinium sp. clade C Y103','Symbiodinium kawagutii','Symbiodinium natans',\n",
    "                   'Symbiodinium sp. CCMP2592','Symbiodinium sp. KB8','Symbiodinium sp. CCMP2456',\n",
    "                   'Symbiodinium pilosum','Symbiodinium necroappetens','Cladocopium goreaui']\n",
    "\n",
    "#for each cluster\n",
    "for cluster in clusters:\n",
    "    all_contigs = {}\n",
    "    concencus_dict = {}\n",
    "    #for file in each cluster\n",
    "    for file in os.listdir(cluster):\n",
    "        filename = file.split('.')[0]\n",
    "        #count_dict has a count of number of each occurence of match\n",
    "        count_dict = {'Breviolum minutum Mf 1.05b.01':0,'Symbiodinium microadriaticum':0,\n",
    "                      'Symbiodinium sp. clade A Y106':0,'Symbiodinium sp. clade C Y103':0,'Symbiodinium kawagutii':0,\n",
    "                      'Symbiodinium natans':0,'Symbiodinium sp. CCMP2592':0,'Symbiodinium sp. KB8':0,\n",
    "                      'Symbiodinium sp. CCMP2456':0,'Symbiodinium pilosum':0,'Symbiodinium necroappetens':0,\n",
    "                      'Cladocopium goreaui':0,'Dendronephthya gigantea':0, 'Xenia sp. Carnegie-2017': 0}\n",
    "        if '.long.tab' in file:\n",
    "            with open(cluster + file, 'r') as f:\n",
    "                #print(filename)\n",
    "                #get header number, get last column\n",
    "                for line in f:\n",
    "                    line = line.split(sep = '\\t')\n",
    "                    #increase count for match\n",
    "                    for entry in count_dict:\n",
    "                        if entry in line[24].strip('\\n'):\n",
    "                            count_dict[entry]+=1\n",
    "            all_contigs[filename] = count_dict\n",
    "            \n",
    "    #get count of octocorral and dinoflagellates\n",
    "    for p in all_contigs:\n",
    "        o = 0\n",
    "        d = 0\n",
    "        for entry in all_contigs[p]:\n",
    "            #print(entry)\n",
    "            if entry in dinoflagellates:\n",
    "                d+=all_contigs[p][entry]\n",
    "            if entry in octocoral:\n",
    "                o+=all_contigs[p][entry]\n",
    "        \n",
    "        #decide (octocoral/dinoflagellates/mixed/none)\n",
    "        decision = 'none'\n",
    "        if d == 0 and o >= 1:\n",
    "            decision = 'octocoral'\n",
    "        if d >= 1 and o ==0 :\n",
    "            decision = 'dinoflagellates'\n",
    "        if d >= 1 and o >= 1:\n",
    "            if (d/o) < threshold:\n",
    "                decision = 'octo'\n",
    "            else:\n",
    "                decision = 'mixed'\n",
    "        #add decision to concensus dict\n",
    "        concencus_dict[p] = decision     \n",
    "    \n",
    "    #print totals of each\n",
    "    octo=dino=mixed=none=0\n",
    "    for key in concencus_dict:\n",
    "        if concencus_dict[key] in 'octocoral':\n",
    "            octo +=1\n",
    "#!!!!!!!!!!!!!!!!Change this to where you want it, if you have other data replace the dicts above too!!!!!!!!!!!!\n",
    "            if n == 0:\n",
    "                needed.append(key)\n",
    "            else:\n",
    "                needed1.append(key)\n",
    "        if concencus_dict[key] in 'dinoflagellates':\n",
    "            dino +=1\n",
    "        if concencus_dict[key] in 'mixed':\n",
    "            mixed +=1\n",
    "        if concencus_dict[key] in 'none':\n",
    "            none +=1\n",
    "#     print('octo: ' + str(octo))\n",
    "#     print('dino: ' + str(dino))\n",
    "#     print('mixed: ' + str(mixed))\n",
    "#     print('none: ' + str(none))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e61d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to output file\n",
    "with open('./out_dir/cluster_info/needed.txt','w') as out:\n",
    "    for i in needed:\n",
    "        out.write(str(i) + '\\n')\n",
    "out.close()\n",
    "print('length in needed.txt: ', len(needed))\n",
    "with open('./out_dir/cluster_info/needed1.txt','w') as out:\n",
    "    for i in needed1:\n",
    "        out.write(str(i) + '\\n')\n",
    "out.close()\n",
    "print('length in needed.txt: ', len(needed1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295ad53",
   "metadata": {},
   "source": [
    "# Blob Plot with Fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc985a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trimmed_df['id'] = int(trimmed_df['id'])\n",
    "with open('out_dir/cluster_info/needed1.txt','r') as keys:\n",
    "    tot = []\n",
    "    for line in keys:\n",
    "        k = line.strip('\\n')\n",
    "        tot.append(k)\n",
    "print('total length: ', len(tot))\n",
    "     \n",
    "#plot blob again and color contigs with keys as labels\n",
    "\n",
    "data = [{'id': k, 'gc_content': v[0], 'length' : v[1]} for k, v in gc_trimmed_mean.items()]\n",
    "# Convert the list of dictionaries to dataframes\n",
    "df1 = pd.DataFrame(data)\n",
    "trimmed_df = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "trimmed_df = trimmed_df[trimmed_df['length'] >= plot_min_length]\n",
    "#print(trimmed_df) \n",
    "\n",
    "#add column if contained in keys\n",
    "trimmed_df['in_fasta'] = trimmed_df['id'].isin(tot)\n",
    "#print(trimmed_df)\n",
    "print('Contigs found in this set also in Blast' + str(trimmed_df.in_fasta.sum()))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "#labels = list(trimmed_df['id'])\n",
    "sc1 = plt.scatter(trimmed_df['average_coverage'], trimmed_df['gc_content'], c = trimmed_df['in_fasta'])\n",
    "cbar = plt.colorbar(sc1)\n",
    "# Add labels and legend\n",
    "plt.xlabel('Trimmed Average Coverage %')\n",
    "plt.ylabel('GC content %')\n",
    "plt.title('Blob plot with trimmed averages with chunks of ' + str(chunk_size))\n",
    "ax.legend([sc1], ['in_fasta'], loc=\"upper right\")\n",
    "\n",
    "# save the plot\n",
    "save_path = os.path.join(out_dir, \"Fasta_Blob.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0742f",
   "metadata": {},
   "source": [
    "## Plot histogram of lengths of this subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520d8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot of lengths of in fasta in trimmed_df\n",
    "data = trimmed_df[trimmed_df[\"in_fasta\"] == True]\n",
    "data = data.sort_values(by=['length'], ascending=False)\n",
    "#x is ids\n",
    "x = data['id']\n",
    "#y is length of ids\n",
    "y = data['length']\n",
    "\n",
    "#color contigs up to total length of genome size\n",
    "current = 0\n",
    "length = genome_size\n",
    "#add lengths together till greater than length\n",
    "for i, l in enumerate(y):\n",
    "    current += l\n",
    "    if current > length:\n",
    "        #print(i)\n",
    "        cutoff = l\n",
    "        break\n",
    "        \n",
    "mask1 = y >= cutoff\n",
    "mask2 = y < cutoff\n",
    "\n",
    "plt.bar(x[mask1], y[mask1], color = 'red')\n",
    "plt.bar(x[mask2], y[mask2], color = 'blue')\n",
    "#plt.bar(x, y)\n",
    "\n",
    "plt.xlabel('Contig ids')\n",
    "plt.ylabel('Lengths')\n",
    "plt.title('Bar plot of contig lengths in Blast results')\n",
    "# save the plot\n",
    "save_path = os.path.join(out_dir, \"Histogram_of_lengths_of_subset.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391bcae",
   "metadata": {},
   "source": [
    "# Manually inspect and set cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c11de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save selected contigs\n",
    "results_cutoff = 190\n",
    "subset = data.head(results_cutoff)\n",
    "#print(subset)\n",
    "subset.to_csv('./out_dir/blast_subset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f27c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each id in blast_subset add that contig to a fasta file\n",
    "ids = []\n",
    "with open('./out_dir/blast_subset.csv','r') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        ids.append(row[0])\n",
    "    \n",
    "output_file = open('./data/ourGenome.fasta', 'a')\n",
    "with open(assembly, 'r') as fasta_file:\n",
    "    header = ''\n",
    "    sequence = ''\n",
    "    for line in fasta_file:\n",
    "        if line.startswith('>'):\n",
    "            #check if id in list\n",
    "            if any(id in line for id in ids):\n",
    "                header = line.strip()\n",
    "        else:\n",
    "            sequence = line.strip()\n",
    "            if header and sequence:\n",
    "                output_file.write(header + '\\n')\n",
    "                output_file.write(sequence + '\\n')\n",
    "                header = ''\n",
    "                sequence = ''\n",
    "                \n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO find lengths of discarded\n",
    "data = [{'id': k, 'gc_content': v[0], 'length' : v[1]} for k, v in gc_trimmed_mean.items()]\n",
    "# Convert the list of dictionaries to dataframes\n",
    "df1 = pd.DataFrame(data)\n",
    "discarded = pd.merge(df1, trimmed_mean, on = 'id')\n",
    "discarded = discarded[discarded['length'] >= plot_min_length]\n",
    "discarded['in_fasta'] = discarded['id'].isin(tot)\n",
    "#print(discarded)\n",
    "\n",
    "#take only false\n",
    "data = discarded[discarded[\"in_fasta\"] == False]\n",
    "data = data.sort_values(by=['length'], ascending=False)\n",
    "#print(data)\n",
    "\n",
    "#x is ids\n",
    "x = data['id']\n",
    "#y is length of ids\n",
    "y = data['length']\n",
    "plt.bar(x,y)\n",
    "\n",
    "plt.xlabel('Contig ids')\n",
    "plt.ylabel('Lengths')\n",
    "plt.title('Bar plot of contig lengths NOT in Blast results')\n",
    "save_path = os.path.join(out_dir, \"Histogram_of_lengths_not_in_Blast_results.png\")\n",
    "plt.savefig(save_path)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9cae43",
   "metadata": {},
   "source": [
    "# Now go on to use the Filtered Results :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cladorhizid",
   "language": "python",
   "name": "cladorhizid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
